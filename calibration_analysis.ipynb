{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6aa0c3",
   "metadata": {},
   "source": [
    "# Probability Calibration Analysis\n",
    "## Hybrid Ensemble Model — Premier League 2023–2025\n",
    "\n",
    "This notebook evaluates **probability calibration** of the hybrid Dixon-Coles / LightGBM ensemble.  \n",
    "A well-calibrated model means: when it predicts a 70% chance of a home win, that outcome should occur approximately 70% of the time.  \n",
    "Poor calibration — even with high accuracy — leads to systematically over- or under-confident predictions, directly harming any downstream decision-making in recruitment or performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2629ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.makedirs('assets', exist_ok=True)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#f8f9fa',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.4,\n",
    "    'grid.linestyle': '--',\n",
    "    'font.family': 'sans-serif',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_dataset.csv')\n",
    "df['match_date'] = pd.to_datetime(df['match_date'])\n",
    "df = df.sort_values('match_date').reset_index(drop=True)\n",
    "\n",
    "# Derive binary target and home-win probability from clean columns\n",
    "df['home_win'] = (df['actual_result'] == 'H').astype(int)\n",
    "df['model_prob_home_win'] = df['prob_H']\n",
    "\n",
    "y = df['home_win'].values\n",
    "p = df['model_prob_home_win'].values\n",
    "\n",
    "print(f\"Loaded {len(df)} matches | Home win rate: {y.mean():.3f} | Mean predicted prob: {p.mean():.3f}\")\n",
    "print(df[['match_date','home_team','away_team','actual_result','predicted_result','correct','prob_H','prob_D','prob_A']].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2c334",
   "metadata": {},
   "source": [
    "## 1. Reliability Curve (Calibration Curve)\n",
    "\n",
    "The diagonal represents perfect calibration. Points above the line indicate the model is **under-confident** (events happen more often than predicted). Points below indicate **over-confidence**.  \n",
    "Quantile-based binning is used to ensure each bin contains a roughly equal number of matches, producing more statistically stable estimates than uniform width bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316db85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_true, prob_pred = calibration_curve(y, p, n_bins=10, strategy='quantile')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='#888888', linewidth=1.5, label='Perfect calibration')\n",
    "ax.plot(prob_pred, prob_true, marker='o', color='#003366', linewidth=2,\n",
    "        markersize=7, label='Ensemble model (Dixon-Coles + LightGBM)')\n",
    "\n",
    "ax.fill_between(prob_pred, prob_pred, prob_true,\n",
    "                where=(prob_true > prob_pred), alpha=0.12, color='#2a9d8f', label='Under-confident')\n",
    "ax.fill_between(prob_pred, prob_pred, prob_true,\n",
    "                where=(prob_true <= prob_pred), alpha=0.12, color='#E63946', label='Over-confident')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Mean Predicted Probability', fontsize=11)\n",
    "ax.set_ylabel('Observed Win Frequency', fontsize=11)\n",
    "ax.set_title('Reliability Curve — Home Win Probability\\n(Premier League 2023–2025)', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/calibration_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: assets/calibration_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd3e092",
   "metadata": {},
   "source": [
    "![Calibration Curve](https://raw.githubusercontent.com/vkenard/football-performance-analytics/main/assets/calibration_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425e3a8",
   "metadata": {},
   "source": [
    "## 3. Brier Score — Measuring Probability Accuracy\n",
    "\n",
    "The **Brier Score** is the mean squared error between predicted probabilities and actual outcomes. Unlike accuracy, it rewards confident *correct* predictions and penalises confident *wrong* ones.\n",
    "\n",
    "$$BS = \\frac{1}{N}\\sum_{i=1}^{N}(p_i - y_i)^2$$\n",
    "\n",
    "- **Range:** 0 (perfect) to 1 (perfectly wrong)\n",
    "- **Baseline:** A naive model predicting the historical mean win rate for every match\n",
    "- **Brier Skill Score (BSS):** How much better the model is vs the naive baseline — positive values confirm the model adds predictive value\n",
    "\n",
    "> A Brier Score improvement over baseline demonstrates that the ensemble captures genuine signal about match outcomes, not just historical averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d52067",
   "metadata": {},
   "outputs": [],
   "source": [
    "brier = brier_score_loss(y, p)\n",
    "\n",
    "# Baseline: predict the global mean win rate for every match\n",
    "baseline_prob = np.full(len(y), y.mean())\n",
    "brier_baseline = brier_score_loss(y, baseline_prob)\n",
    "\n",
    "# Brier Skill Score: how much better than the naive baseline\n",
    "bss = 1 - (brier / brier_baseline)\n",
    "\n",
    "print(\"=\" * 42)\n",
    "print(\"  Brier Score Analysis\")\n",
    "print(\"=\" * 42)\n",
    "print(f\"  Model Brier Score    : {brier:.4f}\")\n",
    "print(f\"  Baseline Brier Score : {brier_baseline:.4f}\")\n",
    "print(f\"  Brier Skill Score    : {bss:+.4f}\")\n",
    "print(\"-\" * 42)\n",
    "if bss > 0:\n",
    "    print(f\"  Model is {bss*100:.1f}% better than naive baseline\")\n",
    "else:\n",
    "    print(\"  Model underperforms naive baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7b326",
   "metadata": {},
   "source": [
    "## 4. Probability Sharpness — Decile Reliability Table\n",
    "\n",
    "A well-calibrated model should show **monotonic alignment** between predicted probability bands and observed frequencies. If matches the model rates as 70–80% likely to produce a home win actually do so ~75% of the time, the probabilities are genuinely informative.\n",
    "\n",
    "This decile table splits all predictions into 10 equally-populated buckets and compares:\n",
    "- **mean_predicted** — average model probability in that bucket\n",
    "- **actual_rate** — true observed frequency in that bucket\n",
    "- **count** — number of matches\n",
    "\n",
    "*Monotonically increasing actual_rate confirms the model discriminates meaningfully between high and low-probability matches.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b46c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cal = df[['model_prob_home_win', 'home_win']].copy()\n",
    "df_cal['decile'] = pd.qcut(df_cal['model_prob_home_win'], q=10, labels=False, duplicates='drop')\n",
    "\n",
    "summary = (\n",
    "    df_cal.groupby('decile')\n",
    "    .agg(\n",
    "        mean_predicted=('model_prob_home_win', 'mean'),\n",
    "        actual_rate=('home_win', 'mean'),\n",
    "        count=('home_win', 'count')\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "summary.index = [f\"D{i+1}\" for i in range(len(summary))]\n",
    "\n",
    "print(summary.to_string())\n",
    "\n",
    "# Bar chart: predicted vs actual by decile\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(summary))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, summary['mean_predicted'], w, label='Mean Predicted', color='#003366', alpha=0.85)\n",
    "ax.bar(x + w/2, summary['actual_rate'], w, label='Actual Rate', color='#E63946', alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary.index, fontsize=9)\n",
    "ax.set_ylabel('Probability / Win Rate')\n",
    "ax.set_xlabel('Probability Decile (D1 = lowest predicted, D10 = highest)')\n",
    "ax.set_title('Predicted vs Actual Win Rate by Probability Decile\\n(Premier League 2023–2025)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/decile_reliability.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: assets/decile_reliability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e52e2",
   "metadata": {},
   "source": [
    "![Decile Reliability](https://raw.githubusercontent.com/vkenard/football-performance-analytics/main/assets/decile_reliability.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
